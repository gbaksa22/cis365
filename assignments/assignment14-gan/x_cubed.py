# -*- coding: utf-8 -*-
"""x_cubed.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hGORAa5LzIKflH6VpbaEEYpI_dNh9zit
"""

# Imports
import numpy as np
from numpy import hstack, ones, zeros
from numpy.random import rand, randn
from keras.models import Sequential
from keras.layers import Dense, Input, Dropout
from matplotlib import pyplot
from tqdm import tqdm
from keras.optimizers import Adam

# Generate n real samples for x^3 with class labels
def generate_real_samples(n):
    X1 = rand(n) * 2 - 1  # Generate inputs in [-1, 1]
    X2 = X1 ** 3          # Apply cubic function
    X1 = X1.reshape(n, 1)
    X2 = X2.reshape(n, 1)
    X = hstack((X1, X2))
    y = ones((n, 1))
    return X, y

# Define the standalone discriminator model
def define_discriminator(n_inputs=2):
    model = Sequential()
    model.add(Input(shape=(n_inputs,)))
    model.add(Dense(50, activation='relu', kernel_initializer='he_uniform'))
    model.add(Dropout(0.3))  # Regularization
    model.add(Dense(25, activation='relu', kernel_initializer='he_uniform'))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0002), metrics=['accuracy'])
    return model


# Define the standalone generator model
def define_generator(latent_dim, n_outputs=2):
    model = Sequential()
    model.add(Input(shape=(latent_dim,)))
    model.add(Dense(50, activation='relu', kernel_initializer='he_uniform'))
    model.add(Dense(25, activation='relu', kernel_initializer='he_uniform'))
    model.add(Dense(n_outputs, activation='tanh'))  # Keep output range [-1, 1]
    return model


# Define the combined generator and discriminator model
def define_gan(generator, discriminator):
    discriminator.trainable = False
    model = Sequential()
    model.add(generator)
    model.add(discriminator)
    model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.0002))
    return model

# Generate points in latent space as input for the generator
def generate_latent_points(latent_dim, n):
    x_input = randn(latent_dim * n)
    x_input = x_input.reshape(n, latent_dim)
    return x_input

# Use the generator to generate n fake examples, with class labels
def generate_fake_samples(generator, latent_dim, n):
    x_input = generate_latent_points(latent_dim, n)
    X = generator.predict(x_input)
    y = zeros((n, 1))  # Label as fake
    return X, y

# Evaluate the discriminator and plot real and fake points
def summarize_performance(epoch, generator, discriminator, latent_dim, n=100):
    x_real, y_real = generate_real_samples(n)
    _, acc_real = discriminator.evaluate(x_real, y_real, verbose=0)
    x_fake, y_fake = generate_fake_samples(generator, latent_dim, n)
    _, acc_fake = discriminator.evaluate(x_fake, y_fake, verbose=0)
    print(f"Epoch {epoch}: Real Accuracy={acc_real:.2f}, Fake Accuracy={acc_fake:.2f}")
    pyplot.scatter(x_real[:, 0], x_real[:, 1], color='red', label='Real')
    pyplot.scatter(x_fake[:, 0], x_fake[:, 1], color='blue', label='Fake')
    pyplot.legend()
    pyplot.show()

# Train the generator and discriminator
def train(g_model, d_model, gan_model, latent_dim, n_epochs=1000, n_batch=128, n_eval=200):
    half_batch = int(n_batch / 2)
    for i in tqdm(range(n_epochs), desc="Training Progress"):
        # Train the discriminator
        x_real, y_real = generate_real_samples(half_batch)
        x_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)
        d_loss_real = d_model.train_on_batch(x_real, y_real)
        d_loss_fake = d_model.train_on_batch(x_fake, y_fake)

        # Train the generator more frequently
        for _ in range(3):  # Train generator 3 times per discriminator step
            x_gan = generate_latent_points(latent_dim, n_batch)
            y_gan = ones((n_batch, 1))  # Labels for generator's output
            g_loss = gan_model.train_on_batch(x_gan, y_gan)

        # Extract scalar loss values
        d_loss_real_value = d_loss_real[0] if isinstance(d_loss_real, list) else d_loss_real
        d_loss_fake_value = d_loss_fake[0] if isinstance(d_loss_fake, list) else d_loss_fake
        g_loss_value = g_loss[0] if isinstance(g_loss, list) else g_loss

        # Log progress
        if (i + 1) % 100 == 0:
            print(f"Epoch {i+1}/{n_epochs}, D Loss Real: {d_loss_real_value:.3f}, "
                  f"D Loss Fake: {d_loss_fake_value:.3f}, G Loss: {g_loss_value:.3f}")
        if (i + 1) % n_eval == 0:
            summarize_performance(i + 1, g_model, d_model, latent_dim)

# Size of the latent space
latent_dim = 5

# Initialize models
discriminator = define_discriminator()
generator = define_generator(latent_dim)
gan_model = define_gan(generator, discriminator)

# Train the GAN
train(generator, discriminator, gan_model, latent_dim, n_epochs=1000, n_batch=128, n_eval=200)